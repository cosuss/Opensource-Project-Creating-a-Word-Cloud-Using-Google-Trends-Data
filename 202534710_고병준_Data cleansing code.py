import pandas as pd
import re
import os
import json
from konlpy.tag import Okt
from typing import List

# 1. í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
okt = Okt() # [cite: 1]

# 2. ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ì •ì˜
# í”„ë¡œì íŠ¸ì˜ ëª©ì ì— ë§ê²Œ ë¶ˆìš©ì–´(ë¶„ì„ì—ì„œ ì œì™¸í•  ë‹¨ì–´)ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ìˆ˜ì •í•˜ì„¸ìš”.
KOREAN_STOP_WORDS = [ # [cite: 2]
    'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ë„',
    'ìœ¼ë¡œ', 'ì—ê²Œ', 'ì—ì„œ', 'ë‹¤', 'ì˜', 'ì¢€', 'ê²ƒ', 'ìˆ˜', 'í• ', 'ê³ ',
    'í•˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ë˜ë‹¤', 'ì´ë‹¤', 'ì•„ë‹ˆë‹¤', 'ë³´ë‹¤', 'í•´ì£¼ë‹¤',
    'ë§', 'ê°™ë‹¤', 'ì‹¶ë‹¤', 'ìš°ë¦¬', 'ë„¤', 'ë‚´', 'ì €', 'ì €í¬', 'ë‚˜', 'ì…ë‹ˆë‹¤',
    # ë‰´ìŠ¤ ì œëª©ì—ì„œ ìì£¼ ë‚˜ì˜¤ëŠ” ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì¶”ê°€
    'ë‰´ìŠ¤', 'ì†ë³´', 'ì˜¤ëŠ˜', 'ì—°í•©', 'ê¸°ì', 'ë‹¨ë…', 'ì¢…í•©', 'ê¸ˆì¼', 'í•´ë‹¹', 'ê´€ë ¨'
]

def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ì—ì„œ URL, íŠ¹ìˆ˜ ë¬¸ì, ìˆ«ì ë“±ì„ ì œê±°í•˜ì—¬ ì •ì œí•©ë‹ˆë‹¤. 
    """
    if not isinstance(text, str):
        return ""

    # 1. URL ì œê±°
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text) # 

    # 2. HTML íƒœê·¸ ë° ì´ë©”ì¼ ì£¼ì†Œ ì œê±°
    text = re.sub('<[^>]*>', '', text) # 
    text = re.sub(r'[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+', '', text) # 

    # 3. íŠ¹ìˆ˜ ë¬¸ì/êµ¬ë‘ì /ìˆ«ì ì œê±° (í•œê¸€, ì˜ì–´, ê³µë°±ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ì œê±°)
    text = re.sub(r'[^ê°€-í£A-Za-z\s]', '', text) # 

    # 4. ì—¬ëŸ¬ ê°œì˜ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì¤„ì„
    text = re.sub(r'\s+', ' ', text).strip() # 

    return text

def tokenize_and_filter(text: str) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬ ëª…ì‚¬ë§Œ ì¶”ì¶œí•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. [cite: 4, 5]
    """
    # í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë¶„í•´í•˜ê³  í’ˆì‚¬ íƒœê¹…
    tokens = okt.pos(text, norm=True, stem=True) # [cite: 5]

    # ëª…ì‚¬(Noun)ë§Œ ì¶”ì¶œí•˜ê³  ë¶ˆìš©ì–´ ë° í•œ ê¸€ì ë‹¨ì–´ ì œê±°
    final_words = [
        word for word, tag in tokens
        if tag == 'Noun' and word not in KOREAN_STOP_WORDS and len(word) > 1
    ] # [cite: 5]

    return final_words

def preprocess_data(directory_path: str) -> List[str]:
    """
    ì§€ì •ëœ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ì½ê³ , ë‰´ìŠ¤ ì œëª©ì„ ì¶”ì¶œí•˜ì—¬ ì •ì œ ë° í† í°í™”í•©ë‹ˆë‹¤.
    """
    all_tokens = []
    
    # 1. ë””ë ‰í† ë¦¬ ë‚´ JSON íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    json_files = [f for f in os.listdir(directory_path) if f.endswith('.json')]
    
    if not json_files:
        print(f"âš ï¸ {directory_path} í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        return []
    
    print(f"ğŸ“° ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

    # 2. íŒŒì¼ë³„ë¡œ ì²˜ë¦¬
    for filename in json_files:
        filepath = os.path.join(directory_path, filename)
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # JSON íŒŒì¼ì—ì„œ 'title' í‚¤ì˜ ê°’ë“¤ë§Œ ì¶”ì¶œ
            titles = [item.get('title', '') for item in data]
            
            total_titles = len(titles)
            processed_count = 0
            
            print(f"   -> íŒŒì¼ '{filename}': {total_titles}ê±´ ì²˜ë¦¬ ì‹œì‘")

            # 3. ì¶”ì¶œëœ ì œëª©ì— ëŒ€í•´ ì •ì œ ë° í† í°í™” ì ìš©
            for text in titles:
                cleaned_text = clean_text(text)
                tokens = tokenize_and_filter(cleaned_text)
                all_tokens.extend(tokens)
                processed_count += 1
            
            print(f"   -> íŒŒì¼ '{filename}': ì²˜ë¦¬ ì™„ë£Œ ({processed_count}ê±´)")

        except FileNotFoundError:
            print(f"Error: File not found at {filepath}")
        except json.JSONDecodeError:
            print(f"Error: JSON decoding failed for {filepath}")
        except Exception as e:
            print(f"Error processing {filepath}: {e}")

    print("\nâœ… ì „ì²´ ë°ì´í„° ì •ì œ ë° í† í°í™” ì™„ë£Œ.")
    return all_tokens

# --- ì‹¤í–‰ ì˜ˆì‹œ ---
if __name__ == "__main__":
    
    # [ì„¤ì •] íŒ€ì›ì˜ ì½”ë“œê°€ ì €ì¥í•œ í´ë” ì´ë¦„
    DATA_DIRECTORY = 'collected_data' 
    
    # ë°ì´í„° ì •ì œ ë° í† í°í™” ì‹¤í–‰
    final_word_list = preprocess_data(DATA_DIRECTORY)

    if final_word_list:
        # ë‹¨ì–´ ëª©ë¡ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„(ì›Œë“œ í´ë¼ìš°ë“œ)ì—ì„œ ì‚¬ìš©
        OUTPUT_FILENAME = 'final_tokenized_words.txt'
        
        # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±ì„ ìœ„í•´ ë‹¨ì–´ì™€ ë¹ˆë„ë¥¼ ì¹´ìš´íŠ¸í•˜ëŠ” ì¶”ê°€ ë¡œì§ì´ í•„ìš”í•˜ì§€ë§Œ, 
        # ì—¬ê¸°ì„œëŠ” ì¼ë‹¨ ë¦¬ìŠ¤íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f: # [cite: 12]
            f.write('\n'.join(final_word_list))
        
        print("\n--- ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ---")
        print(f"ì´ ì¶”ì¶œëœ ë‹¨ì–´ ê°œìˆ˜: {len(final_word_list)}ê°œ")
        print(f"ê²°ê³¼ê°€ '{OUTPUT_FILENAME}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ìµœì¢… ë‹¨ì–´ ëª©ë¡ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìˆ˜ì§‘ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")