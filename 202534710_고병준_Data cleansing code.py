import re
import os
import json
from typing import List

KOREAN_STOP_WORDS = [
    'ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì—','ì™€','ê³¼','ë„',
    'ìœ¼ë¡œ','ì—ê²Œ','ì—ì„œ','ë‹¤','ì˜','ì¢€','ê²ƒ','ìˆ˜','í• ','ê³ ',
    'í•˜ë‹¤','ìˆë‹¤','ì—†ë‹¤','ë˜ë‹¤','ì´ë‹¤','ì•„ë‹ˆë‹¤','ë³´ë‹¤','í•´ì£¼ë‹¤',
    'ë§','ê°™ë‹¤','ì‹¶ë‹¤','ìš°ë¦¬','ë„¤','ë‚´','ì €','ì €í¬','ë‚˜','ì…ë‹ˆë‹¤',
    'ë‰´ìŠ¤','ì†ë³´','ì˜¤ëŠ˜','ì—°í•©','ê¸°ì','ë‹¨ë…','ì¢…í•©','ê¸ˆì¼','í•´ë‹¹','ê´€ë ¨'
]

def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = re.sub(r'http[s]?://\S+', '', text)
    text = re.sub('<[^>]*>', '', text)
    text = re.sub(r'[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+', '', text)
    text = re.sub(r'[^ê°€-í£A-Za-z\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def tokenize_konlpy(text: str) -> List[str]:
    from konlpy.tag import Okt
    okt = Okt()
    tokens = okt.pos(text, norm=True, stem=True)
    return [
        w for w, t in tokens
        if t == "Noun" and w not in KOREAN_STOP_WORDS and len(w) > 1
    ]

def tokenize_fallback(text: str) -> List[str]:
    # konlpyê°€ ì•ˆ ë  ë•Œ: ê³µë°± ê¸°ì¤€ + í•œê¸€/ì˜ë¬¸ë§Œ ë‚¨ê¸´ ë’¤ stopword ì œê±°
    words = text.split()
    words = [w for w in words if w not in KOREAN_STOP_WORDS and len(w) > 1]
    return words

def preprocess_data(directory_path: str) -> List[str]:
    if not os.path.isdir(directory_path):
        print(f"âŒ í´ë” ì—†ìŒ: {directory_path}")
        return []

    json_files = [f for f in os.listdir(directory_path) if f.endswith(".json")]
    if not json_files:
        print(f"âš ï¸ {directory_path} í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return []

    # konlpy ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬
    use_konlpy = True
    try:
        import konlpy  # noqa
        from konlpy.tag import Okt  # noqa
    except Exception as e:
        use_konlpy = False
        print("âš ï¸ konlpy ì‚¬ìš© ë¶ˆê°€ â†’ fallback í† í¬ë‚˜ì´ì €ë¡œ ì§„í–‰")
        print("   ì´ìœ :", repr(e))

    all_tokens: List[str] = []
    print(f"ğŸ“° ì´ {len(json_files)}ê°œ JSON ì²˜ë¦¬")

    for filename in json_files:
        filepath = os.path.join(directory_path, filename)
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)

        titles = [item.get("title", "") for item in data if isinstance(item, dict)]
        print(f" - {filename}: {len(titles)}ê±´")

        for t in titles:
            cleaned = clean_text(t)
            if not cleaned:
                continue
            if use_konlpy:
                tokens = tokenize_konlpy(cleaned)
            else:
                tokens = tokenize_fallback(cleaned)
            all_tokens.extend(tokens)

    print(f"âœ… ì´ í† í° ìˆ˜: {len(all_tokens)}")
    return all_tokens

if __name__ == "__main__":
    DATA_DIRECTORY = "collected_data"
    OUTPUT_FILENAME = "final_tokenized_words.txt"

    words = preprocess_data(DATA_DIRECTORY)
    if not words:
        print("âŒ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ (ìˆ˜ì§‘ ë°ì´í„°/í´ë” í™•ì¸)")
        raise SystemExit(1)

    with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
        f.write("\n".join(words))

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_FILENAME}")